# -*- coding: utf-8 -*-
"""HR management .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HlBwQdqGOjXizuI__pFigNgXr0RDprUG
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns

employee = pd.read_csv("Human_Resources.csv")

employee

employee.info()

employee.describe()

employee.head()

# Let's replace the 'Attritition','overtime' and Over18 column with integers before performing any visualizations 
employee['Attrition'] = employee['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)
employee['OverTime'] = employee['OverTime'].apply(lambda x: 1 if x == 'Yes' else 0)
employee['Over18'] = employee['Over18'].apply(lambda x: 1 if x == 'Y' else 0)

employee

#Checking for NULL values 
employee.isnull()
#There are no Null values present in the dataset

employee.hist(bins = 30, figsize = (30,30), color = 'b')

#lets drop Standard Hours , Employee Count , Overtime, Employee number 
employee.drop(['EmployeeCount', 'StandardHours', 'Over18', 'EmployeeNumber'], axis=1, inplace=True)

#Checking the number of people that stayed in the company and how many left 
plt.figure(figsize=(15,10))
ax = sns.countplot(x = 'Attrition',data=employee,palette = 'viridis')
for p in ax.patches:
        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))

"""Number of employees who stayed at the company is 1233 and number of employess who left the company are 237 """

#Making the coorealtion plot for the dataset 
correlations = employee.corr()
f, ax = plt.subplots(figsize = (20, 20))
sns.heatmap(correlations, annot = True)

"""From the above correlation plot we can obsereve that Job Level and Working Hours are strongly coorelated.
Monthly Income is strongly correlated with Job Level and other 
Age is stongly coorelated with Monthly Income   
"""

plt.figure(figsize=[25, 25])
sns.countplot(x = 'Age', hue = 'Attrition', data = employee)

plt.figure(figsize=[20,20])
plt.subplot(411)
sns.countplot(x = 'JobRole', hue = 'Attrition', data = employee)
plt.subplot(412)
sns.countplot(x = 'MaritalStatus', hue = 'Attrition', data = employee)
plt.subplot(413)
sns.countplot(x = 'JobInvolvement', hue = 'Attrition', data = employee)
plt.subplot(414)
sns.countplot(x = 'JobLevel', hue = 'Attrition', data = employee)

# Single employees tend to leave compared to married and divorced
# Sales Representitives tend to leave compared to any other job 
# Less involved employees tend to leave the company 
# Less experienced (low job level) tend to leave the company

# Let's see the Gender vs. Monthly Income
plt.figure(figsize=(15, 10))
sns.boxplot(x = 'MonthlyIncome', y = 'Gender', data = employee)

# Let's see the monthly income vs. job role
plt.figure(figsize=(15, 10))
sns.barplot(x = 'MonthlyIncome', y = 'JobRole', data = employee)

X_cat = employee[['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']]
X_cat

from sklearn.preprocessing import OneHotEncoder
onehotencoder = OneHotEncoder()
X_cat = onehotencoder.fit_transform(X_cat).toarray()

X_cat.shape

X_cat = pd.DataFrame(X_cat)

#Dropping the Attrides Column 
X_numerical = employee[['Age', 'DailyRate', 'DistanceFromHome',	'Education', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement',	'JobLevel',	'JobSatisfaction',	'MonthlyIncome',	'MonthlyRate',	'NumCompaniesWorked',	'OverTime',	'PercentSalaryHike', 'PerformanceRating',	'RelationshipSatisfaction',	'StockOptionLevel',	'TotalWorkingYears'	,'TrainingTimesLastYear'	, 'WorkLifeBalance',	'YearsAtCompany'	,'YearsInCurrentRole', 'YearsSinceLastPromotion',	'YearsWithCurrManager']]
X_numerical

#Combing both the data frames into one 
X_all = pd.concat([X_cat, X_numerical], axis = 1)
X_all

#Normalizing the data 
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X_all)

X

y = employee['Attrition']
y

#Splitting the dataset into training and testing 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

#Importing all the necessary libraries for classification 
import tensorflow as tf
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense,Activation,Dropout
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xg

#Logistic Regression 
classifier = LogisticRegression(random_state=0, max_iter=2000)
classifier.fit(X_train, y_train)

y_pred1 = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred1)
print(cm)
accuracy_score(y_test, y_pred1)

"""Logistic Regression Shows an accuracy of 88%"""

#Decsion Tree 
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

y_pred1 = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred1)
print(cm)
accuracy_score(y_test, y_pred1)

"""Decsion Tree Shows An accuracy of 78.8%"""

#K Nearest Neighbours 
classifier = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""KNN Shows an accuracy 84.2%"""

#Naive Bayes 
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
1 - accuracy_score(y_test, y_pred)

"""Naive Bayes has an accuracy of 30%"""

#Support Vector Classifier 
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""Support Vector Classifier has an accuracy of 87.5%"""

#Random Forest Classifier 
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""Random Forest Classifier has accuracy of 84.5%"""

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=500, activation='relu', input_shape=(50, )))
model.add(tf.keras.layers.Dense(units=500, activation='relu'))
model.add(tf.keras.layers.Dense(units=500, activation='relu'))
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

model.summary()

model.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])

epochs_hist = model.fit(X_train, y_train,validation_data=(X_test,y_test), epochs = 10, batch_size = 50)

"""The neural net give us a training accuracy of 94.8% and testing accuracy of 87%




"""

#Gradient Boostong 
classifier = GradientBoostingClassifier(n_estimators = 10, random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""Gradient Boosting gave us an accuracy of 83.4%"""

#XG Boosting 
classifier = xg.XGBClassifier(use_label_encoder=False, eval_metric = 'error')
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""XGBoost Gave an accuracy of 86.6%"""

